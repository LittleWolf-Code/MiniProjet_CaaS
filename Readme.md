# üó≥Ô∏è MiniProjet CaaS ‚Äî Pipeline CI/CD pour Docker Voting App

**Module :** Cloud as a Service (CaaS)  
**Projet :** Pipeline CI/CD ‚Äî Docker Voting App (Microservices)  
**Date :** 12/02/2026  
**√âtudiant :** Lopvet Lucas

---

## üìë Table des mati√®res

| Section | Contenu | Dur√©e |
|---------|---------|:-----:|
| [0. Discussion](#0--discussion-sur-lid√©e-de-lapplication) | Architecture, langages, BDD | 1‚Äì2 min |
| [1. D√©monstration](#1--d√©monstration-end-to-end) | Pipeline Jenkins ‚Üí Docker Hub ‚Üí K8s ‚Üí Grafana | 7‚Äì8 min |
| [2. Justification](#2--justification--explication) | Jenkinsfile, Manifests K8s, Prometheus/Grafana | 3‚Äì4 min |
| [Annexe A](#annexe-a--installation-compl√®te-sur-ubuntu) | Guide d'installation Ubuntu | ‚Äî |
| [Annexe B](#annexe-b--d√©pannage) | D√©pannage | ‚Äî |

---

# 0 ‚Äî Discussion sur l'id√©e de l'application

## Application choisie : Docker Voting App

Nous utilisons la **Docker Voting App**, application officielle de d√©monstration cr√©√©e par Docker :  
üîó [github.com/dockersamples/example-voting-app](https://github.com/dockersamples/example-voting-app)

C'est une application de **sondage en temps r√©el** : l'utilisateur vote pour **¬´ Cats ¬ª** ou **¬´ Dogs ¬ª**, et les r√©sultats s'affichent instantan√©ment.

## Architecture : Microservices (5 composants)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           DOCKER VOTING APP                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ   üó≥Ô∏è VOTE      ‚îÇ                           ‚îÇ   üìä RESULT    ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  (Python Flask) ‚îÇ                           ‚îÇ   (Node.js)    ‚îÇ          ‚îÇ
‚îÇ   ‚îÇ  Port: 80       ‚îÇ                           ‚îÇ   Port: 80     ‚îÇ          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ           ‚îÇ                                            ‚îÇ                    ‚îÇ
‚îÇ           ‚îÇ √©crit les votes                            ‚îÇ lit les r√©sultats  ‚îÇ
‚îÇ           ‚ñº                                            ‚îÇ                    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇ   üî¥ REDIS     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ‚öôÔ∏è WORKER    ‚îÇ          ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇ   (Cache/Queue) ‚îÇ       ‚îÇ   (.NET Core)  ‚îÇ          ‚îÇ                    ‚îÇ
‚îÇ   ‚îÇ   Port: 6379   ‚îÇ       ‚îÇ                ‚îÇ          ‚îÇ                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ                    ‚îÇ
‚îÇ                                    ‚îÇ                    ‚îÇ                    ‚îÇ
‚îÇ                                    ‚îÇ √©crit              ‚îÇ lit                ‚îÇ
‚îÇ                                    ‚ñº                    ‚ñº                    ‚îÇ
‚îÇ                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ                           ‚îÇ   üêò POSTGRESQL               ‚îÇ                  ‚îÇ
‚îÇ                           ‚îÇ   (Base de donn√©es)           ‚îÇ                  ‚îÇ
‚îÇ                           ‚îÇ   Port: 5432                  ‚îÇ                  ‚îÇ
‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Flux de donn√©es :**
```
Utilisateur ‚îÄ‚îÄ‚ñ∂ vote (Flask) ‚îÄ‚îÄ‚ñ∂ Redis ‚îÄ‚îÄ‚ñ∂ Worker (.NET) ‚îÄ‚îÄ‚ñ∂ PostgreSQL ‚îÄ‚îÄ‚ñ∂ result (Node.js) ‚îÄ‚îÄ‚ñ∂ Utilisateur
```

## Langages / Frameworks

| Service | Langage | Framework | R√¥le |
|---------|---------|-----------|------|
| **vote** | Python | Flask + Gunicorn | Interface web de vote |
| **result** | Node.js | Express + Socket.io | Affichage des r√©sultats en temps r√©el (WebSocket) |
| **worker** | C# (.NET 7) | ‚Äî | Traitement des votes (Redis ‚Üí PostgreSQL) |

## Bases de donn√©es ‚Äî Justification du choix

| Base | R√¥le | Pourquoi ce choix |
|------|------|-------------------|
| **Redis** | File d'attente temporaire (cache) | Ultra-rapide en m√©moire, parfait pour du stockage temporaire de votes en attente. Structure de donn√©es `list` (RPUSH/LPOP) id√©ale pour une file d'attente |
| **PostgreSQL** | Stockage permanent des votes | Base relationnelle robuste, support ACID pour la persistance des donn√©es. Requ√™tes SQL pour l'agr√©gation des r√©sultats (`COUNT`, `GROUP BY`) |

> **Pourquoi 2 bases ?** Redis sert de **tampon** entre le frontend (vote) et le backend (worker), ce qui d√©couple l'√©criture de la lecture. Le worker traite les votes √† son rythme et les persiste dans PostgreSQL. Cela garantit la **r√©silience** (les votes ne sont pas perdus si le worker red√©marre).

## Architecture CI/CD

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     git push     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     webhook     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ D√©veloppeur ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   GitHub    ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   Jenkins   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                        ‚îÇ
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ  1. Checkout
                                       ‚îÇ  2. Build images
                                       ‚îÇ  3. Push images
                                       ‚ñº
                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                ‚îÇ Docker Hub  ‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚îÇ 4. kubectl apply
                                       ‚ñº
                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                ‚îÇ Kubernetes  ‚îÇ ‚óÄ‚îÄ‚îÄ Prometheus + Grafana
                                ‚îÇ  (Minikube) ‚îÇ     (Monitoring)
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Structure du d√©p√¥t

```
MiniProjet_CaaS/
‚îú‚îÄ‚îÄ vote/                          # Microservice VOTE (Python Flask)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ static/stylesheets/
‚îÇ   ‚îî‚îÄ‚îÄ templates/index.html
‚îú‚îÄ‚îÄ result/                        # Microservice RESULT (Node.js)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ server.js
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ views/
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ worker/                        # Microservice WORKER (.NET Core)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ Program.cs
‚îÇ   ‚îî‚îÄ‚îÄ Worker.csproj
‚îú‚îÄ‚îÄ k8s/                           # Manifests Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ vote-deployment.yaml       # 2 replicas, 250m CPU, 128Mi RAM
‚îÇ   ‚îú‚îÄ‚îÄ vote-service.yaml          # NodePort 31000
‚îÇ   ‚îú‚îÄ‚îÄ result-deployment.yaml     # 1 replica, 250m CPU, 128Mi RAM
‚îÇ   ‚îú‚îÄ‚îÄ result-service.yaml        # NodePort 31001
‚îÇ   ‚îú‚îÄ‚îÄ worker-deployment.yaml     # 1 replica, 500m CPU, 256Mi RAM
‚îÇ   ‚îú‚îÄ‚îÄ redis-deployment.yaml      # 1 replica
‚îÇ   ‚îú‚îÄ‚îÄ redis-service.yaml         # ClusterIP
‚îÇ   ‚îú‚îÄ‚îÄ db-deployment.yaml         # 1 replica, PostgreSQL 15
‚îÇ   ‚îî‚îÄ‚îÄ db-service.yaml            # ClusterIP
‚îú‚îÄ‚îÄ jenkins/                       # Jenkins dockeris√©
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Jenkins + Docker CLI + kubectl
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml         # Jenkins + DinD
‚îú‚îÄ‚îÄ Jenkinsfile                    # Pipeline CI/CD (4 stages)
‚îî‚îÄ‚îÄ Readme.md                     # Ce fichier
```

---

# 1 ‚Äî D√©monstration end-to-end

> **Dur√©e : 7‚Äì8 minutes.** Cette section montre que toute la cha√Æne fonctionne.

## 1.1 ‚Äî Lancer / Montrer la pipeline Jenkins

### Acc√©der √† Jenkins

```bash
# Jenkins est accessible via le navigateur
# http://localhost:8080
```

Montrer :
1. Le **job `MiniProjet-CaaS`** dans le dashboard
2. Cliquer sur **Build Now** pour lancer un build (ou montrer le dernier run r√©ussi)
3. Montrer la **Stage View** avec les 4 stages r√©ussis (vert)
4. Cliquer sur le build ‚Üí **Console Output** ‚Üí montrer le r√©sultat `SUCCESS`

> üì∏ **CAPTURE D'√âCRAN 1 :** Dashboard Jenkins avec le job MiniProjet-CaaS
>
> ![Jenkins Dashboard](image/capture_jenkins_dashboard.png)

> üì∏ **CAPTURE D'√âCRAN 2 :** Pipeline Jenkins ‚Äî Stage View avec les 4 stages r√©ussis
>
> ![Jenkins Pipeline](image/capture_jenkins_pipeline.png)

> üì∏ **CAPTURE D'√âCRAN 3 :** Console Output Jenkins ‚Äî R√©sultat SUCCESS avec les logs
>
> ![Jenkins Console](image/capture_jenkins_console.png)

---

## 1.2 ‚Äî Montrer les images sur Docker Hub

### V√©rifier les images Docker Hub

```bash
# Ouvrir Docker Hub dans le navigateur :
# https://hub.docker.com/u/litlewolf

# Ou v√©rifier en ligne de commande :
docker images | grep litlewolf
```

Montrer sur Docker Hub pour chaque image (`litlewolf/vote`, `litlewolf/result`, `litlewolf/worker`) :
- Le **tag** (`latest` + num√©ro de build)
- La **date** de push
- Le **digest** (SHA256)

> üì∏ **CAPTURE D'√âCRAN 4 :** Page Docker Hub avec les 3 images (tag, date, digest visibles)
>
> ![Docker Hub Images](image/capture_dockerhub_images.png)

---

## 1.3 ‚Äî Montrer Kubernetes

### V√©rifier les pods, services et d√©ploiements

```bash
# V√©rifier que tous les pods tournent
kubectl get pods

# R√©sultat attendu : tous les pods en Running
# NAME                      READY   STATUS    RESTARTS   AGE
# db-xxxxx                  1/1     Running   0          XXm
# redis-xxxxx               1/1     Running   0          XXm
# vote-xxxxx                1/1     Running   0          XXm
# vote-yyyyy                1/1     Running   0          XXm  (2 replicas)
# result-xxxxx              1/1     Running   0          XXm
# worker-xxxxx              1/1     Running   0          XXm
```

```bash
# V√©rifier les services expos√©s
kubectl get svc

# R√©sultat attendu :
# NAME    TYPE        CLUSTER-IP     PORT(S)
# vote    NodePort    10.x.x.x       5000:31000/TCP
# result  NodePort    10.x.x.x       5001:31001/TCP
# redis   ClusterIP   10.x.x.x       6379/TCP
# db      ClusterIP   10.x.x.x       5432/TCP
```

```bash
# V√©rifier les d√©ploiements
kubectl get deploy

# R√©sultat attendu :
# NAME     READY   UP-TO-DATE   AVAILABLE
# vote     2/2     2            2
# result   1/1     1            1
# worker   1/1     1            1
# redis    1/1     1            1
# db       1/1     1            1
```

> üì∏ **CAPTURE D'√âCRAN 5 :** R√©sultat de `kubectl get pods` ‚Äî tous les pods en Running
>
> ![Kubectl Pods](image/capture_kubectl_pods.png)

> üì∏ **CAPTURE D'√âCRAN 6 :** R√©sultat de `kubectl get svc` ‚Äî services avec leurs ports
>
> ![Kubectl Services](image/capture_kubectl_services.png)

> üì∏ **CAPTURE D'√âCRAN 7 :** R√©sultat de `kubectl get deploy` ‚Äî tous les d√©ploiements READY
>
> ![Kubectl Deployments](image/capture_kubectl_deploy.png)

### Acc√©der √† l'application

```bash
# Obtenir l'URL du service vote
minikube service vote --url
# ‚Üí Ouvre http://<minikube-ip>:31000

# Obtenir l'URL du service result
minikube service result --url
# ‚Üí Ouvre http://<minikube-ip>:31001
```

| Service | URL |
|---------|-----|
| **Vote** (voter) | `http://<MINIKUBE_IP>:31000` |
| **Result** (r√©sultats) | `http://<MINIKUBE_IP>:31001` |

**D√©monstration live :**
1. Ouvrir l'interface **vote** ‚Üí voter pour ¬´ Cats ¬ª ou ¬´ Dogs ¬ª
2. Ouvrir l'interface **result** ‚Üí constater que le vote appara√Æt **en temps r√©el**
3. Voter plusieurs fois ‚Üí les pourcentages changent en direct

> üì∏ **CAPTURE D'√âCRAN 8 :** Interface de vote ‚Äî page web Cats vs Dogs
>
> ![Interface Vote](image/capture_vote_app.png)

> üì∏ **CAPTURE D'√âCRAN 9 :** Page des r√©sultats ‚Äî mise √† jour en temps r√©el
>
> ![Interface Result](image/capture_result_app.png)

---

## 1.4 ‚Äî Montrer le monitoring (Grafana)

### Acc√©der √† Grafana

```bash
# Exposer Grafana via port-forward (si pas d√©j√† fait)
kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 &
```

Ouvrir : **http://localhost:3000** (login: `admin` / `admin`)

### Dashboard √† montrer

Aller dans **Dashboards** et montrer :
- **CPU par pod** dans le namespace `default`
- **M√©moire par pod**
- **Nombre de pods en running**

> üì∏ **CAPTURE D'√âCRAN 10 :** Dashboard Grafana ‚Äî m√©triques CPU/m√©moire des pods de l'application
>
> ![Grafana Dashboard](image/capture_grafana_dashboard.png)

> üì∏ **CAPTURE D'√âCRAN 11 :** Prometheus ‚Äî requ√™te ex√©cut√©e (optionnel)
>
> ![Prometheus](image/capture_prometheus.png)

---

# 2 ‚Äî Justification / explication

> **Dur√©e : 3‚Äì4 minutes.** Expliquer rapidement les fichiers de configuration cl√©s.

## 2.1 ‚Äî Le Jenkinsfile

Le fichier `Jenkinsfile` √† la racine du projet d√©finit la pipeline CI/CD en **4 stages** :

```groovy
pipeline {
    agent any

    environment {
        DOCKERHUB_USER = 'litlewolf'
        VOTE_IMAGE     = "${DOCKERHUB_USER}/vote"
        RESULT_IMAGE   = "${DOCKERHUB_USER}/result"
        WORKER_IMAGE   = "${DOCKERHUB_USER}/worker"
        BUILD_TAG      = "${env.BUILD_NUMBER}"
    }

    stages {
        stage('Checkout')           { ... } // 1. R√©cup√©ration du code depuis GitHub
        stage('Build Docker Images') { ... } // 2. Construction des 3 images Docker
        stage('Push to Docker Hub')  { ... } // 3. Push vers Docker Hub avec credentials
        stage('Deploy to Kubernetes'){ ... } // 4. D√©ploiement via kubectl apply -f k8s/
    }
}
```

### Explication des points cl√©s

| √âl√©ment | Explication |
|---------|------------|
| **`agent any`** | La pipeline peut s'ex√©cuter sur n'importe quel agent Jenkins disponible |
| **`environment`** | Variables globales : user Docker Hub, noms d'images, tag = num√©ro de build |
| **`BUILD_TAG`** | Chaque build g√©n√®re un tag unique (ex: `litlewolf/vote:3`) + `latest` |
| **`withCredentials`** | Utilise les credentials `dockerhub-credentials` stock√©s dans Jenkins pour le `docker login` |
| **`kubectl apply -f k8s/`** | Applique tous les manifests Kubernetes d'un coup |
| **`kubectl rollout status`** | Attend que chaque d√©ploiement soit termin√© avant de continuer |

### S√©curit√©
- Les identifiants Docker Hub sont stock√©s dans Jenkins (pas dans le code)
- Le `docker logout` est fait syst√©matiquement dans le bloc `post > always`

---

## 2.2 ‚Äî Les manifests Kubernetes

Les 9 fichiers dans `k8s/` d√©finissent **5 Deployments** et **4 Services** :

### D√©ploiements

| Deployment | Image | Replicas | CPU | M√©moire | Pourquoi |
|------------|-------|:--------:|:---:|:-------:|----------|
| **vote** | `litlewolf/vote:latest` | **2** | 250m | 128Mi | 2 replicas pour la haute disponibilit√© (frontend principal) |
| **result** | `litlewolf/result:latest` | 1 | 250m | 128Mi | 1 replica suffit (affichage uniquement) |
| **worker** | `litlewolf/worker:latest` | 1 | 500m | 256Mi | Plus de ressources car traitement de donn√©es (Redis ‚Üí PostgreSQL) |
| **redis** | `redis:alpine` | 1 | 250m | 128Mi | Image officielle l√©g√®re Alpine |
| **db** | `postgres:15-alpine` | 1 | 500m | 256Mi | PostgreSQL 15 avec `emptyDir` pour le volume (donn√©es √©ph√©m√®res pour la d√©mo) |

### Services

| Service | Type | Port externe | Port interne | Pourquoi |
|---------|------|:------------:|:------------:|----------|
| **vote** | **NodePort** | 31000 | 80 | Accessible depuis l'ext√©rieur du cluster pour voter |
| **result** | **NodePort** | 31001 | 80 | Accessible depuis l'ext√©rieur pour voir les r√©sultats |
| **redis** | ClusterIP | ‚Äî | 6379 | Accessible uniquement au sein du cluster (communication interne) |
| **db** | ClusterIP | ‚Äî | 5432 | Accessible uniquement au sein du cluster (communication interne) |

> **Pourquoi NodePort ?** Minikube ne supporte pas le type `LoadBalancer` nativement. NodePort expose un port fixe sur l'IP du n≈ìud, accessible via `minikube service`.

> **Pas d'Ingress** car pour une d√©mo locale avec Minikube, NodePort est suffisant et plus simple √† mettre en place.

---

## 2.3 ‚Äî Prometheus / Grafana : comment c'est branch√©

### Installation via Helm

```bash
# Le chart kube-prometheus-stack installe tout d'un coup :
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin
```

### Ce que le chart installe

| Composant | R√¥le |
|-----------|------|
| **Prometheus** | Collecte les m√©triques ‚Äî scrape automatiquement tous les pods/nodes Kubernetes |
| **Grafana** | Visualisation ‚Äî pr√©configur√©e avec Prometheus comme datasource |
| **AlertManager** | Alertes (non utilis√© dans cette d√©mo) |
| **kube-state-metrics** | Expose les m√©triques des objets Kubernetes (pods, deployments, services) |
| **node-exporter** | Expose les m√©triques syst√®me des n≈ìuds (CPU, RAM, disque) |

### Comment le scrape fonctionne

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     scrape /metrics     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Prometheus  ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ kube-state-metrics‚îÇ  (m√©triques K8s)
‚îÇ              ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ node-exporter     ‚îÇ  (m√©triques syst√®me)
‚îÇ              ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ kubelet/cAdvisor  ‚îÇ  (m√©triques conteneurs)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ datasource auto-configur√©e
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Grafana    ‚îÇ ‚Üí Dashboards pr√©-install√©s + import de dashboards personnalis√©s
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **Prometheus** scrape automatiquement les endpoints gr√¢ce aux `ServiceMonitor` CRDs install√©s par le chart Helm
- **Grafana** est pr√©configur√©e avec la datasource Prometheus ‚Äî pas de configuration manuelle n√©cessaire
- Les dashboards import√©s (ID `15661`, `6417`, `315`) fournissent des vues pr√™tes √† l'emploi

### Requ√™tes Prometheus utiles

```promql
# Utilisation CPU par pod (namespace default = nos pods applicatifs)
sum(rate(container_cpu_usage_seconds_total{namespace="default"}[5m])) by (pod)

# M√©moire utilis√©e par pod
sum(container_memory_usage_bytes{namespace="default"}) by (pod)

# Nombre de pods en running
count(kube_pod_status_phase{phase="Running", namespace="default"})
```

---

# Annexe A ‚Äî Installation compl√®te sur Ubuntu

> **Toutes les commandes ci-dessous sont pour Ubuntu (22.04 LTS recommand√©).**  
> La machine doit disposer d'au moins **8 Go de RAM** et **4 CPU**.

## A.1 ‚Äî Pr√©requis (installation des outils)

### Docker

```bash
# Installer Docker
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Ajouter l'utilisateur au groupe docker
sudo usermod -aG docker $USER
newgrp docker

# V√©rifier
docker --version
```

### Minikube

```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
rm minikube-linux-amd64

# V√©rifier
minikube version
```

### kubectl

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
rm kubectl

# V√©rifier
kubectl version --client
```

### Helm

```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# V√©rifier
helm version
```

### V√©rification rapide de tous les outils

```bash
git --version
docker --version
minikube version
kubectl version --client
helm version
```

> üì∏ **CAPTURE D'√âCRAN :** R√©sultat de la v√©rification des outils (toutes les versions affich√©es)
>
> ![V√©rification des outils](image/capture_outils.png)

---

## A.2 ‚Äî Cloner le d√©p√¥t

```bash
git clone https://github.com/LittleWolf-Code/MiniProjet_CaaS.git
cd MiniProjet_CaaS
git checkout projet-final
ls -la
```

---

## A.3 ‚Äî D√©marrer Minikube

```bash
# D√©marrer le cluster Minikube
minikube start --driver=docker --cpus=4 --memory=4096

# V√©rifier
kubectl cluster-info
kubectl get nodes
```

---

## A.4 ‚Äî Construire et pousser les images Docker (optionnel, si pas de Jenkins)

> ‚ö†Ô∏è **Remplacez `litlewolf` par votre identifiant Docker Hub si diff√©rent.**

```bash
export DOCKERHUB_USER="litlewolf"

# Build des 3 images
docker build -t $DOCKERHUB_USER/vote:latest ./vote
docker build -t $DOCKERHUB_USER/result:latest ./result
docker build -t $DOCKERHUB_USER/worker:latest ./worker

# Push vers Docker Hub
docker login
docker push $DOCKERHUB_USER/vote:latest
docker push $DOCKERHUB_USER/result:latest
docker push $DOCKERHUB_USER/worker:latest
```

---

## A.5 ‚Äî Installer Jenkins (dockeris√©)

```bash
# Cr√©er le r√©seau minikube si n√©cessaire
docker network create minikube 2>/dev/null || true

# Lancer Jenkins + DinD avec docker compose
cd jenkins
docker compose up -d
cd ..

# R√©cup√©rer le mot de passe admin initial
docker exec jenkins-blueocean cat /var/jenkins_home/secrets/initialAdminPassword
```

Acc√©der √† Jenkins via : **http://localhost:8080**

### Configurer Jenkins

1. **Installer les plugins sugg√©r√©s** (lors de la premi√®re connexion)
2. **Ajouter les credentials Docker Hub :**
   - Aller dans **Manage Jenkins** ‚Üí **Credentials** ‚Üí **(global)** ‚Üí **Add Credentials**
   - Kind : `Username with password`
   - Username : votre identifiant Docker Hub
   - Password : votre mot de passe Docker Hub
   - ID : `dockerhub-credentials`

3. **Cr√©er la pipeline :**
   - **New Item** ‚Üí Nom : `MiniProjet-CaaS` ‚Üí Type : **Pipeline**
   - Pipeline ‚Üí Definition : `Pipeline script from SCM`
   - SCM : Git
   - Repository URL : `https://github.com/LittleWolf-Code/MiniProjet_CaaS.git`
   - Branch : `*/projet-final`
   - Script Path : `Jenkinsfile`
   - **Save**

4. **Lancer le build :** Cliquer sur **Build Now**

---

## A.6 ‚Äî D√©ployer sur Kubernetes

```bash
# Appliquer tous les manifests
kubectl apply -f k8s/

# Attendre que tous les pods soient pr√™ts
kubectl rollout status deployment/vote
kubectl rollout status deployment/result
kubectl rollout status deployment/worker
kubectl rollout status deployment/redis
kubectl rollout status deployment/db

# V√©rifier
kubectl get pods
kubectl get svc
kubectl get deploy
```

### Acc√©der √† l'application

```bash
minikube service vote --url
# ‚Üí http://<minikube-ip>:31000

minikube service result --url
# ‚Üí http://<minikube-ip>:31001
```

---

## A.7 ‚Äî Installer le monitoring (Prometheus + Grafana)

```bash
# Ajouter le repo Helm
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Installer la stack compl√®te
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin

# Attendre que les pods soient pr√™ts (~2-3 minutes)
kubectl get pods -n monitoring -w
```

### Acc√©der √† Grafana

```bash
kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 &
```

Ouvrir : **http://localhost:3000** (login : `admin` / `admin`)

### Importer un dashboard

1. **Dashboards** ‚Üí **Import**
2. Entrer l'ID : **`15661`** (Kubernetes Cluster Monitoring)
3. **Load** ‚Üí s√©lectionner la datasource **Prometheus** ‚Üí **Import**

| ID | Dashboard recommand√© |
|----|--------------------|
| **15661** | Kubernetes Cluster Monitoring |
| **6417** | Kubernetes Pods Monitoring |
| **315** | Kubernetes Cluster Overview |

### Acc√©der √† Prometheus (optionnel)

```bash
kubectl port-forward -n monitoring svc/monitoring-kube-prometheus-prometheus 9090:9090 &
```

Ouvrir : **http://localhost:9090**

---

# Annexe B ‚Äî D√©pannage

### Pods en CrashLoopBackOff

```bash
kubectl logs <nom-du-pod>
kubectl describe pod <nom-du-pod>
```

### Minikube ne d√©marre pas

```bash
minikube delete
minikube start --driver=docker --cpus=4 --memory=4096
```

### Docker permission denied

```bash
sudo usermod -aG docker $USER
newgrp docker
```

### Jenkins ne peut pas acc√©der √† Docker

```bash
# Si Jenkins dockeris√© : v√©rifier que DinD tourne
docker ps | grep jenkins-docker

# Si Jenkins natif :
sudo usermod -aG docker jenkins
sudo systemctl restart jenkins
```

### R√©initialiser le d√©ploiement Kubernetes

```bash
kubectl delete -f k8s/
kubectl apply -f k8s/
```

---

## üåê R√©sum√© des ports et acc√®s

| Service | Type | Port | URL |
|---------|------|:----:|-----|
| **Vote** (frontend) | NodePort | 31000 | `http://<MINIKUBE_IP>:31000` |
| **Result** (frontend) | NodePort | 31001 | `http://<MINIKUBE_IP>:31001` |
| **Redis** | ClusterIP | 6379 | Interne uniquement |
| **PostgreSQL** | ClusterIP | 5432 | Interne uniquement |
| **Jenkins** | Docker | 8080 | `http://localhost:8080` |
| **Grafana** | Port-forward | 3000 | `http://localhost:3000` |
| **Prometheus** | Port-forward | 9090 | `http://localhost:9090` |

```bash
# R√©cup√©rer l'IP de Minikube
minikube ip
```

---

## üéØ Conclusion

Ce projet d√©montre la mise en place d'une **cha√Æne DevOps compl√®te** :

| √âtape | R√©alisation |
|-------|------------|
| **1. Code source** | ‚úÖ Repository GitHub avec code source structur√© (3 microservices) |
| **2. Dockerisation** | ‚úÖ 3 images Docker (vote, result, worker) construites et pouss√©es sur Docker Hub |
| **3. CI/CD** | ‚úÖ Pipeline Jenkins automatis√©e (Checkout ‚Üí Build ‚Üí Push ‚Üí Deploy) |
| **4. Orchestration** | ‚úÖ D√©ploiement Kubernetes avec 5 services sur Minikube |
| **5. Monitoring** | ‚úÖ Prometheus + Grafana pour la supervision du cluster |

**Technologies utilis√©es :** Git, GitHub, Docker, Docker Hub, Jenkins, Kubernetes (Minikube), Helm, Prometheus, Grafana

---

> **Auteur :** Lopvet Lucas  
> **Module :** Cloud as a Service (CaaS)  
> **Date :** 12/02/2026