# ğŸ—³ï¸ MiniProjet CaaS â€” Pipeline CI/CD pour Docker Voting App

**Module :** Cloud as a Service (CaaS)  
**Projet :** Pipeline CI/CD â€” Docker Voting App (Microservices)  
**Date :** 12/02/2026  
**Ã‰tudiant :** Lopvet Lucas

---

## ğŸ“‘ Table des matiÃ¨res

| Section | Contenu | DurÃ©e soutenance |
|---------|---------|:-----:|
| [0. Discussion](#0--discussion-sur-lidÃ©e-de-lapplication) | Architecture, langages, BDD | 1â€“2 min |
| [1. DÃ©monstration](#1--dÃ©monstration-end-to-end) | Pipeline Jenkins â†’ Docker Hub â†’ K8s â†’ Grafana | 7â€“8 min |
| [2. Justification](#2--justification--explication) | Jenkinsfile, Manifests K8s, Prometheus/Grafana | 3â€“4 min |
| [Guide de rÃ©alisation](#-guide-de-rÃ©alisation-pas-Ã -pas) | Toutes les commandes depuis le git clone | â€” |

---

# 0 â€” Discussion sur l'idÃ©e de l'application

## Application choisie : Docker Voting App

Nous utilisons la **Docker Voting App**, application officielle de dÃ©monstration crÃ©Ã©e par Docker :  
ğŸ”— [github.com/dockersamples/example-voting-app](https://github.com/dockersamples/example-voting-app)

C'est une application de **sondage en temps rÃ©el** : l'utilisateur vote pour **Â« Cats Â»** ou **Â« Dogs Â»**, et les rÃ©sultats s'affichent instantanÃ©ment.

## Architecture : Microservices (5 composants)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DOCKER VOTING APP                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚   ğŸ—³ï¸ VOTE      â”‚                           â”‚   ğŸ“Š RESULT    â”‚          â”‚
â”‚   â”‚  (Python Flask) â”‚                           â”‚   (Node.js)    â”‚          â”‚
â”‚   â”‚  Port: 80       â”‚                           â”‚   Port: 80     â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                                            â”‚                    â”‚
â”‚           â”‚ Ã©crit les votes                            â”‚ lit les rÃ©sultats  â”‚
â”‚           â–¼                                            â”‚                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚                    â”‚
â”‚   â”‚   ğŸ”´ REDIS     â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   âš™ï¸ WORKER    â”‚          â”‚                    â”‚
â”‚   â”‚   (Cache/Queue) â”‚       â”‚   (.NET Core)  â”‚          â”‚                    â”‚
â”‚   â”‚   Port: 6379   â”‚       â”‚                â”‚          â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚                    â”‚
â”‚                                    â”‚                    â”‚                    â”‚
â”‚                                    â”‚ Ã©crit              â”‚ lit                â”‚
â”‚                                    â–¼                    â–¼                    â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                           â”‚   ğŸ˜ POSTGRESQL               â”‚                  â”‚
â”‚                           â”‚   (Base de donnÃ©es)           â”‚                  â”‚
â”‚                           â”‚   Port: 5432                  â”‚                  â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flux de donnÃ©es :**
```
Utilisateur â”€â”€â–¶ vote (Flask) â”€â”€â–¶ Redis â”€â”€â–¶ Worker (.NET) â”€â”€â–¶ PostgreSQL â”€â”€â–¶ result (Node.js) â”€â”€â–¶ Utilisateur
```

## Langages / Frameworks

| Service | Langage | Framework | RÃ´le |
|---------|---------|-----------|------|
| **vote** | Python | Flask + Gunicorn | Interface web de vote |
| **result** | Node.js | Express + Socket.io | Affichage des rÃ©sultats en temps rÃ©el (WebSocket) |
| **worker** | C# (.NET 7) | â€” | Traitement des votes (Redis â†’ PostgreSQL) |

## Bases de donnÃ©es â€” Justification du choix

| Base | RÃ´le | Pourquoi ce choix |
|------|------|-------------------|
| **Redis** | File d'attente temporaire (cache) | Ultra-rapide en mÃ©moire, parfait pour du stockage temporaire de votes en attente. Structure `list` (RPUSH/LPOP) idÃ©ale pour une file d'attente |
| **PostgreSQL** | Stockage permanent des votes | Base relationnelle robuste, support ACID pour la persistance. RequÃªtes SQL pour l'agrÃ©gation des rÃ©sultats (`COUNT`, `GROUP BY`) |

> **Pourquoi 2 bases ?** Redis sert de **tampon** entre le frontend (vote) et le backend (worker), ce qui dÃ©couple l'Ã©criture de la lecture. Le worker traite les votes Ã  son rythme et les persiste dans PostgreSQL. Cela garantit la **rÃ©silience** (les votes ne sont pas perdus si le worker redÃ©marre).

## Architecture CI/CD

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     git push     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     webhook     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DÃ©veloppeur â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   GitHub    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   Jenkins   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                        â”‚
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚  1. Checkout
                                       â”‚  2. Build images
                                       â”‚  3. Push images
                                       â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚ Docker Hub  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ 4. kubectl apply
                                       â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚ Kubernetes  â”‚ â—€â”€â”€ Prometheus + Grafana
                                â”‚  (Minikube) â”‚     (Monitoring)
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 1 â€” DÃ©monstration end-to-end

> **DurÃ©e : 7â€“8 minutes.** Cette section montre que toute la chaÃ®ne fonctionne.

## 1.1 â€” Lancer / Montrer la pipeline Jenkins

AccÃ©der Ã  Jenkins : **http://localhost:8080**

Montrer :
1. Le **job `MiniProjet-CaaS`** dans le dashboard
2. Cliquer sur **Build Now** pour lancer un build (ou montrer le dernier run rÃ©ussi)
3. Montrer la **Stage View** avec les 4 stages rÃ©ussis (vert)
4. Cliquer sur le build â†’ **Console Output** â†’ montrer le rÃ©sultat `SUCCESS`

> ğŸ“¸ **CAPTURE 1 :** Dashboard Jenkins avec le job
>
> ![Jenkins Dashboard](image/capture_jenkins_dashboard.png)

> ğŸ“¸ **CAPTURE 2 :** Stage View â€” 4 stages rÃ©ussis
>
> ![Jenkins Pipeline](image/capture_jenkins_pipeline.png)

> ğŸ“¸ **CAPTURE 3 :** Console Output â€” SUCCESS
>
> ![Jenkins Console](image/capture_jenkins_console.png)

---

## 1.2 â€” Montrer les images sur Docker Hub

```bash
# Ouvrir https://hub.docker.com/u/litlewolf
# Ou vÃ©rifier en ligne de commande :
docker images | grep litlewolf
```

Montrer pour chaque image (`litlewolf/vote`, `litlewolf/result`, `litlewolf/worker`) :
- Le **tag** (`latest` + numÃ©ro de build)
- La **date** de push
- Le **digest** (SHA256)

> ğŸ“¸ **CAPTURE 4 :** Docker Hub â€” les 3 images (tag, date, digest)
>
> ![Docker Hub](image/capture_dockerhub_images.png)

---

## 1.3 â€” Montrer Kubernetes

```bash
kubectl get pods
kubectl get svc
kubectl get deploy
```

RÃ©sultats attendus : tous les pods en **Running**, 2 replicas pour vote, services NodePort sur 31000 et 31001.

```bash
# AccÃ©der Ã  l'application
minikube service vote --url      # â†’ http://<IP>:31000
minikube service result --url    # â†’ http://<IP>:31001
```

**DÃ©monstration live :** Voter pour Cats/Dogs â†’ voir les rÃ©sultats en temps rÃ©el.

> ğŸ“¸ **CAPTURE 5 :** `kubectl get pods` â€” tous en Running
>
> ![Kubectl Pods](image/capture_kubectl_pods.png)

> ğŸ“¸ **CAPTURE 6 :** `kubectl get svc` + `kubectl get deploy`
>
> ![Kubectl Services](image/capture_kubectl_services.png)

> ğŸ“¸ **CAPTURE 7 :** Interface de vote Cats vs Dogs
>
> ![Vote App](image/capture_vote_app.png)

> ğŸ“¸ **CAPTURE 8 :** Page des rÃ©sultats en temps rÃ©el
>
> ![Result App](image/capture_result_app.png)

---

## 1.4 â€” Montrer le monitoring (Grafana)

AccÃ©der Ã  Grafana : **http://localhost:3000** (login : `admin` / `admin`)

Montrer les dashboards avec mÃ©triques CPU / mÃ©moire des pods.

> ğŸ“¸ **CAPTURE 9 :** Dashboard Grafana â€” mÃ©triques CPU/mÃ©moire
>
> ![Grafana](image/capture_grafana_dashboard.png)

---

# 2 â€” Justification / explication

> **DurÃ©e : 3â€“4 minutes.**

## 2.1 â€” Le Jenkinsfile

```groovy
pipeline {
    agent any

    environment {
        DOCKERHUB_USER = 'litlewolf'
        VOTE_IMAGE     = "${DOCKERHUB_USER}/vote"
        RESULT_IMAGE   = "${DOCKERHUB_USER}/result"
        WORKER_IMAGE   = "${DOCKERHUB_USER}/worker"
        BUILD_TAG      = "${env.BUILD_NUMBER}"
    }

    stages {
        stage('Checkout')            { ... } // 1. RÃ©cupÃ¨re le code depuis GitHub
        stage('Build Docker Images') { ... } // 2. Build les 3 images Docker
        stage('Push to Docker Hub')  { ... } // 3. Push avec credentials Jenkins
        stage('Deploy to Kubernetes'){ ... } // 4. kubectl apply -f k8s/
    }
}
```

| Ã‰lÃ©ment | Explication |
|---------|------------|
| **`agent any`** | S'exÃ©cute sur n'importe quel agent Jenkins |
| **`BUILD_TAG`** | Tag unique par build (ex: `litlewolf/vote:3`) + `latest` |
| **`withCredentials`** | Credentials `dockerhub-credentials` stockÃ©s dans Jenkins (pas en clair) |
| **`kubectl apply -f k8s/`** | Applique tous les manifests K8s d'un coup |
| **`kubectl rollout status`** | Attend la fin du dÃ©ploiement avant de passer au suivant |
| **`docker logout`** | Toujours exÃ©cutÃ© (bloc `post > always`) pour la sÃ©curitÃ© |

---

## 2.2 â€” Les manifests Kubernetes

### DÃ©ploiements

| Deployment | Image | Replicas | CPU | MÃ©moire | Pourquoi |
|------------|-------|:--------:|:---:|:-------:|----------|
| **vote** | `litlewolf/vote:latest` | **2** | 250m | 128Mi | 2 replicas pour la haute disponibilitÃ© (frontend) |
| **result** | `litlewolf/result:latest` | 1 | 250m | 128Mi | 1 replica suffit (lecture seule) |
| **worker** | `litlewolf/worker:latest` | 1 | 500m | 256Mi | Plus de ressources (traitement Redis â†’ PG) |
| **redis** | `redis:alpine` | 1 | 250m | 128Mi | Image officielle Alpine (lÃ©gÃ¨re) |
| **db** | `postgres:15-alpine` | 1 | 500m | 256Mi | `emptyDir` pour le volume (dÃ©mo) |

### Services

| Service | Type | Port externe | Pourquoi |
|---------|------|:------------:|----------|
| **vote** | **NodePort** | 31000 | Accessible depuis l'extÃ©rieur pour voter |
| **result** | **NodePort** | 31001 | Accessible pour voir les rÃ©sultats |
| **redis** | ClusterIP | â€” | Communication interne uniquement |
| **db** | ClusterIP | â€” | Communication interne uniquement |

> **NodePort** car Minikube ne supporte pas `LoadBalancer` nativement. Pas d'Ingress car NodePort suffit pour une dÃ©mo locale.

---

## 2.3 â€” Prometheus / Grafana

### Installation via Helm (kube-prometheus-stack)

Le chart Helm `kube-prometheus-stack` installe **tout automatiquement** :

| Composant | RÃ´le |
|-----------|------|
| **Prometheus** | Scrape automatiquement tous les pods/nodes K8s |
| **Grafana** | PrÃ©configurÃ©e avec Prometheus comme datasource |
| **kube-state-metrics** | MÃ©triques des objets K8s (pods, deploys) |
| **node-exporter** | MÃ©triques systÃ¨me des nÅ“uds (CPU, RAM) |

### Comment le scrape fonctionne

```
Prometheus â—€â”€â”€ scrape â”€â”€ kube-state-metrics  (mÃ©triques K8s)
           â—€â”€â”€ scrape â”€â”€ node-exporter       (mÃ©triques systÃ¨me)
           â—€â”€â”€ scrape â”€â”€ kubelet/cAdvisor    (mÃ©triques conteneurs)
     â”‚
     â–¼ datasource auto-configurÃ©e
  Grafana â†’ Dashboards prÃ©-installÃ©s + import ID 15661/6417/315
```

- Les `ServiceMonitor` CRDs configurent automatiquement les cibles de scrape
- Grafana est prÃ©configurÃ©e â€” aucune configuration manuelle nÃ©cessaire
- Dashboards importÃ©s par ID pour des vues prÃªtes Ã  l'emploi

---

# ğŸš€ Guide de rÃ©alisation pas Ã  pas

> **Ce guide suppose une machine Ubuntu avec les outils suivants dÃ©jÃ  installÃ©s :**  
> Git, Docker, Minikube, kubectl, Jenkins  
> La machine doit avoir au moins **8 Go de RAM** et **4 CPU**.

---

## Ã‰tape 1 â€” VÃ©rifier les outils prÃ©-installÃ©s

```bash
git --version
docker --version
minikube version
kubectl version --client
```

VÃ©rifier que Docker fonctionne sans sudo :

```bash
docker ps
```

> Si Â« permission denied Â» :
> ```bash
> sudo usermod -aG docker $USER
> newgrp docker
> ```

> ğŸ“¸ **CAPTURE :** Versions de tous les outils affichÃ©es
>
> ![Outils](image/capture_outils.png)

---

## Ã‰tape 2 â€” Cloner le dÃ©pÃ´t et se placer sur la branche

```bash
cd ~
git clone https://github.com/LittleWolf-Code/MiniProjet_CaaS.git
cd MiniProjet_CaaS
git checkout projet-final
```

VÃ©rifier la structure :

```bash
ls -la
# Attendu : vote/  result/  worker/  k8s/  jenkins/  Jenkinsfile  Readme.md
```

> ğŸ“¸ **CAPTURE :** Repository clonÃ© + `ls -la`
>
> ![Structure](image/capture_structure_projet.png)

---

## Ã‰tape 3 â€” DÃ©marrer Minikube

```bash
minikube start --driver=docker --cpus=4 --memory=4096
```

VÃ©rifier :

```bash
kubectl cluster-info
kubectl get nodes
```

> ğŸ“¸ **CAPTURE :** Minikube dÃ©marrÃ© + `kubectl get nodes`
>
> ![Minikube](image/capture_minikube_start.png)

---

## Ã‰tape 4 â€” Construire et pousser les images Docker manuellement

> âš ï¸ **Remplacez `litlewolf` par votre identifiant Docker Hub si diffÃ©rent.**

```bash
export DOCKERHUB_USER="litlewolf"

# Build des 3 images
docker build -t $DOCKERHUB_USER/vote:latest ./vote
docker build -t $DOCKERHUB_USER/result:latest ./result
docker build -t $DOCKERHUB_USER/worker:latest ./worker
```

VÃ©rifier :

```bash
docker images | grep $DOCKERHUB_USER
```

Pousser vers Docker Hub :

```bash
docker login
# Entrer votre identifiant et mot de passe Docker Hub

docker push $DOCKERHUB_USER/vote:latest
docker push $DOCKERHUB_USER/result:latest
docker push $DOCKERHUB_USER/worker:latest
```

> ğŸ“¸ **CAPTURE :** `docker images` montrant les 3 images
>
> ![Docker Images](image/capture_docker_images.png)

> ğŸ“¸ **CAPTURE :** Push rÃ©ussi vers Docker Hub
>
> ![Docker Push](image/capture_docker_push.png)

---

## Ã‰tape 5 â€” DÃ©ployer sur Kubernetes

```bash
# Appliquer tous les manifests d'un coup
kubectl apply -f k8s/
```

Attendre que tout soit prÃªt :

```bash
kubectl rollout status deployment/vote
kubectl rollout status deployment/result
kubectl rollout status deployment/worker
kubectl rollout status deployment/redis
kubectl rollout status deployment/db
```

VÃ©rifier :

```bash
kubectl get pods
kubectl get svc
kubectl get deploy
```

> Tous les pods doivent Ãªtre en Ã©tat **Running** (attendre 1-2 min si nÃ©cessaire).

AccÃ©der Ã  l'application :

```bash
minikube service vote --url
# â†’ Ouvrir l'URL dans le navigateur (port 31000)

minikube service result --url
# â†’ Ouvrir l'URL (port 31001)
```

**Tester :** Voter pour Cats ou Dogs â†’ vÃ©rifier les rÃ©sultats en temps rÃ©el.

> ğŸ“¸ **CAPTURE :** `kubectl get pods` â€” tous Running
>
> ![Pods](image/capture_kubectl_pods.png)

> ğŸ“¸ **CAPTURE :** `kubectl get svc` + `kubectl get deploy`
>
> ![Services](image/capture_kubectl_services.png)

> ğŸ“¸ **CAPTURE :** Interface de vote
>
> ![Vote](image/capture_vote_app.png)

> ğŸ“¸ **CAPTURE :** RÃ©sultats en temps rÃ©el
>
> ![Result](image/capture_result_app.png)

---

## Ã‰tape 6 â€” Installer Jenkins dockerisÃ©

```bash
# CrÃ©er le rÃ©seau minikube pour que Jenkins puisse communiquer avec le cluster
docker network create minikube 2>/dev/null || true
```

```bash
# Lancer Jenkins + Docker-in-Docker
cd ~/MiniProjet_CaaS/jenkins
docker compose up -d
```

VÃ©rifier que les conteneurs tournent :

```bash
docker ps | grep jenkins
# Attendu : jenkins-blueocean (Jenkins) + jenkins-docker (DinD)
```

RÃ©cupÃ©rer le mot de passe admin initial :

```bash
# Attendre ~30 secondes que Jenkins dÃ©marre
sleep 30
docker exec jenkins-blueocean cat /var/jenkins_home/secrets/initialAdminPassword
```

Ouvrir Jenkins dans le navigateur : **http://localhost:8080**

1. Coller le mot de passe admin initial
2. Choisir **Â« Install suggested plugins Â»** â†’ attendre l'installation
3. CrÃ©er un utilisateur admin (ou continuer avec admin)

> ğŸ“¸ **CAPTURE :** Jenkins dashboard aprÃ¨s installation
>
> ![Jenkins](image/capture_jenkins_dashboard.png)

---

## Ã‰tape 7 â€” Configurer les credentials Docker Hub dans Jenkins

1. Dans Jenkins, aller dans **Manage Jenkins** â†’ **Credentials**
2. Cliquer sur **(global)** â†’ **Add Credentials**
3. Remplir :

| Champ | Valeur |
|-------|--------|
| **Kind** | Username with password |
| **Username** | Votre identifiant Docker Hub |
| **Password** | Votre mot de passe Docker Hub |
| **ID** | `dockerhub-credentials` |
| **Description** | Docker Hub Credentials |

4. Cliquer **Create**

> ğŸ“¸ **CAPTURE :** Credentials Docker Hub configurÃ©s
>
> ![Credentials](image/capture_jenkins_credentials.png)

---

## Ã‰tape 8 â€” Copier la config kubectl dans Jenkins

```bash
# Copier la config kubeconfig dans le conteneur Jenkins
docker cp ~/.kube/config jenkins-blueocean:/home/jenkins/.kube/config

# Corriger les permissions
docker exec -u root jenkins-blueocean chown -R jenkins:jenkins /home/jenkins/.kube
```

VÃ©rifier que Jenkins peut accÃ©der au cluster :

```bash
docker exec jenkins-blueocean kubectl get nodes
# Attendu : le nÅ“ud minikube affichÃ©
```

> **Si Ã§a ne marche pas :** le problÃ¨me est souvent que l'adresse du cluster dans kubeconfig pointe vers `127.0.0.1` mais Jenkins est dans un conteneur Docker. Solution :
>
> ```bash
> # RÃ©cupÃ©rer l'IP de minikube accessible depuis le rÃ©seau Docker
> MINIKUBE_IP=$(minikube ip)
>
> # Remplacer l'adresse dans la config copiÃ©e
> docker exec jenkins-blueocean sed -i "s|https://127.0.0.1:[0-9]*|https://$MINIKUBE_IP:8443|g" /home/jenkins/.kube/config
> docker exec jenkins-blueocean sed -i "s|certificate-authority: .*|insecure-skip-tls-verify: true|g" /home/jenkins/.kube/config
>
> # Re-tester
> docker exec jenkins-blueocean kubectl get nodes
> ```

---

## Ã‰tape 9 â€” CrÃ©er et lancer la pipeline Jenkins

1. Dans Jenkins, cliquer sur **New Item**
2. Nom : `MiniProjet-CaaS`
3. Type : **Pipeline**
4. Configuration :
   - **Pipeline** â†’ **Definition** : `Pipeline script from SCM`
   - **SCM** : Git
   - **Repository URL** : `https://github.com/LittleWolf-Code/MiniProjet_CaaS.git`
   - **Branch** : `*/projet-final`
   - **Script Path** : `Jenkinsfile`
5. Cliquer **Save**

6. Cliquer **Build Now** â†’ observer les 4 stages :
   - âœ… Checkout
   - âœ… Build Docker Images
   - âœ… Push to Docker Hub
   - âœ… Deploy to Kubernetes

> ğŸ“¸ **CAPTURE :** Pipeline Jenkins â€” 4 stages rÃ©ussis (Stage View)
>
> ![Pipeline](image/capture_jenkins_pipeline.png)

> ğŸ“¸ **CAPTURE :** Console Output â€” SUCCESS
>
> ![Console](image/capture_jenkins_console.png)

---

## Ã‰tape 10 â€” Installer Helm

```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version
```

---

## Ã‰tape 11 â€” Installer le monitoring (Prometheus + Grafana)

```bash
# Ajouter le repo Helm de Prometheus
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Installer la stack complÃ¨te
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin
```

Attendre que tous les pods soient prÃªts (~2-3 minutes) :

```bash
kubectl get pods -n monitoring
# Attendre que tout soit Running (relancer si nÃ©cessaire)
```

---

## Ã‰tape 12 â€” AccÃ©der Ã  Grafana

```bash
# Exposer Grafana
kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 &
```

Ouvrir : **http://localhost:3000**

| Champ | Valeur |
|-------|--------|
| **Login** | `admin` |
| **Mot de passe** | `admin` |

### Importer un dashboard

1. **Dashboards** â†’ **Import**
2. Entrer l'ID : **`15661`** â†’ **Load** â†’ sÃ©lectionner datasource **Prometheus** â†’ **Import**

Dashboards recommandÃ©s :

| ID | Dashboard |
|----|---------:|
| **15661** | Kubernetes Cluster Monitoring |
| **6417** | Kubernetes Pods Monitoring |
| **315** | Kubernetes Cluster Overview |

> ğŸ“¸ **CAPTURE :** Dashboard Grafana avec mÃ©triques CPU/mÃ©moire
>
> ![Grafana](image/capture_grafana_dashboard.png)

### AccÃ©der Ã  Prometheus (optionnel)

```bash
kubectl port-forward -n monitoring svc/monitoring-kube-prometheus-prometheus 9090:9090 &
```

Ouvrir : **http://localhost:9090**

RequÃªtes utiles :

```promql
# CPU par pod
sum(rate(container_cpu_usage_seconds_total{namespace="default"}[5m])) by (pod)

# MÃ©moire par pod
sum(container_memory_usage_bytes{namespace="default"}) by (pod)
```

> ğŸ“¸ **CAPTURE :** Prometheus avec requÃªte exÃ©cutÃ©e
>
> ![Prometheus](image/capture_prometheus.png)

---

## ğŸŒ RÃ©sumÃ© des ports et accÃ¨s

| Service | Type | Port | URL |
|---------|------|:----:|-----|
| **Vote** | NodePort | 31000 | `http://<MINIKUBE_IP>:31000` |
| **Result** | NodePort | 31001 | `http://<MINIKUBE_IP>:31001` |
| **Redis** | ClusterIP | 6379 | Interne |
| **PostgreSQL** | ClusterIP | 5432 | Interne |
| **Jenkins** | Docker | 8080 | `http://localhost:8080` |
| **Grafana** | Port-forward | 3000 | `http://localhost:3000` |
| **Prometheus** | Port-forward | 9090 | `http://localhost:9090` |

```bash
minikube ip    # RÃ©cupÃ©rer l'IP de Minikube
```

---

## ğŸ”§ DÃ©pannage

### Pods en CrashLoopBackOff

```bash
kubectl logs <nom-du-pod>
kubectl describe pod <nom-du-pod>
```

### Minikube ne dÃ©marre pas

```bash
minikube delete
minikube start --driver=docker --cpus=4 --memory=4096
```

### Docker permission denied

```bash
sudo usermod -aG docker $USER
newgrp docker
```

### Jenkins ne peut pas accÃ©der Ã  Docker (DinD)

```bash
# VÃ©rifier que le conteneur DinD tourne
docker ps | grep jenkins-docker

# Si non, relancer :
cd ~/MiniProjet_CaaS/jenkins
docker compose down
docker compose up -d
```

### Jenkins ne peut pas accÃ©der Ã  Kubernetes

```bash
# Re-copier la config kubectl
docker cp ~/.kube/config jenkins-blueocean:/home/jenkins/.kube/config
docker exec -u root jenkins-blueocean chown -R jenkins:jenkins /home/jenkins/.kube

# Adapter l'IP si nÃ©cessaire
MINIKUBE_IP=$(minikube ip)
docker exec jenkins-blueocean sed -i "s|https://127.0.0.1:[0-9]*|https://$MINIKUBE_IP:8443|g" /home/jenkins/.kube/config
docker exec jenkins-blueocean sed -i "s|certificate-authority: .*|insecure-skip-tls-verify: true|g" /home/jenkins/.kube/config
```

### RÃ©initialiser tout le dÃ©ploiement

```bash
kubectl delete -f k8s/
kubectl apply -f k8s/
```

### Relancer le monitoring

```bash
helm uninstall monitoring -n monitoring
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set grafana.adminPassword=admin
```

---

## ğŸ¯ Conclusion

Ce projet dÃ©montre la mise en place d'une **chaÃ®ne DevOps complÃ¨te** :

| Ã‰tape | RÃ©alisation |
|-------|------------|
| **Code source** | âœ… Repository GitHub avec 3 microservices structurÃ©s |
| **Dockerisation** | âœ… 3 images Docker construites et poussÃ©es sur Docker Hub |
| **CI/CD** | âœ… Pipeline Jenkins (Checkout â†’ Build â†’ Push â†’ Deploy) |
| **Orchestration** | âœ… Kubernetes avec 5 services sur Minikube |
| **Monitoring** | âœ… Prometheus + Grafana via Helm |

**Technologies :** Git, GitHub, Docker, Docker Hub, Jenkins, Kubernetes (Minikube), Helm, Prometheus, Grafana

---

> **Auteur :** Lopvet Lucas  
> **Module :** Cloud as a Service (CaaS)  
> **Date :** 12/02/2026